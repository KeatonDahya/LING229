{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMf2yr/805a7KMzM7Vn0QgX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KeatonDahya/LING229/blob/main/Assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dllDPny2gYN-",
        "outputId": "97230329-ae5c-4dc7-e540-7fcc39af5277"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Set the folder path\n",
        "folder_path = '/content/drive/My Drive/Charles_Dickens_Corpus'\n"
      ],
      "metadata": {
        "id": "oULRoRZVhg0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = \"\"\n",
        "\n",
        "# Read each .txt file in the folder\n",
        "for filename in os.listdir(folder_path):\n",
        "    if filename.endswith('.txt'):\n",
        "        file_path = os.path.join(folder_path, filename)\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            corpus += file.read()\n"
      ],
      "metadata": {
        "id": "Wo6CscKxhpl9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "The corpus consists of ten novels written by Charles Dickens, a prominent 19th-century author known for his exploration of social issues, vivid characterizations, and detailed depictions of Victorian England. The texts include A Tale of Two Cities, Great Expectations, Oliver Twist, David Copperfield, and other notable works. These novels cover themes such as poverty, social inequality, family, and redemption, making them ideal for linguistic and thematic analysis.\n",
        "\n",
        "The texts were sourced from Project Gutenberg, an online repository of public domain books. Each novel was downloaded in .txt format and compiled into a single folder. The files were then uploaded to Google Drive for easy access and integration into the Python environment. This process ensures that the corpus is well-organized and readily available for computational analysis using tools like NLTK in Google Colab."
      ],
      "metadata": {
        "id": "2Puj9UI1iS92"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Basic Metrics"
      ],
      "metadata": {
        "id": "z10NNu9gcJEG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Token and Type Calculation\n",
        "This section calculates the total number of tokens and types in the corpus.\n",
        "\n",
        "- **Tokens**: The total number of words, including repeated words, in the entire corpus.\n",
        "- **Types**: The total number of unique words in the corpus, excluding repetitions."
      ],
      "metadata": {
        "id": "ZkWzxJoiT0mv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import NLTK library\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Tokenize the text into words\n",
        "tokens = nltk.word_tokenize(corpus)\n",
        "\n",
        "# Calculate total tokens\n",
        "total_tokens = len(tokens)\n",
        "\n",
        "# Calculate total types (unique words)\n",
        "unique_types = len(set(tokens))\n",
        "\n",
        "print(f\"Total Tokens: {total_tokens}\")\n",
        "print(f\"Total Types: {unique_types}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jx905grXmYLX",
        "outputId": "e4cf8141-b4e2-4de8-f28a-b95612df761d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Tokens: 3248315\n",
            "Total Types: 54781\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Token and Type Analysis for Each Text\n",
        "\n",
        "This section calcualtes the total number of tokens and types for each individual text in the corpus.\n",
        "\n",
        "\n",
        "\n",
        "*   **Tokens**: The total nmumber of words, including repeated words, in each text.\n",
        "*   **Types:** The total number of unique words in each text, excluding repetitions\n",
        "\n"
      ],
      "metadata": {
        "id": "EOaldMNsYLII"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_metrics = {}\n",
        "\n",
        "for filename in os.listdir(folder_path):\n",
        "    if filename.endswith('.txt'):\n",
        "        file_path = os.path.join(folder_path, filename)\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            text = file.read()  le\n",
        "\n",
        "            # Tokenize the text\n",
        "            tokens = nltk.word_tokenize(text)\n",
        "\n",
        "            # Calculate total tokens and types\n",
        "            total_tokens = len(tokens)\n",
        "            unique_types = len(set(tokens))\n",
        "\n",
        "            # Store the metrics in the dictionary\n",
        "            file_metrics[filename] = {\n",
        "                'Total Tokens': total_tokens,\n",
        "                'Unique Types': unique_types\n",
        "            }\n",
        "\n",
        "\n",
        "for filename, metrics in file_metrics.items():\n",
        "    print(f\"File: {filename}\")\n",
        "    print(f\"  Total Tokens: {metrics['Total Tokens']}\")\n",
        "    print(f\"  Unique Types: {metrics['Unique Types']}\")\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TIenRePMnThe",
        "outputId": "786d5efb-c13e-45fc-fad1-13ba6a77d16a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File: a_tale_of_two_cities.txt\n",
            "  Total Tokens: 169837\n",
            "  Unique Types: 11884\n",
            "\n",
            "File: bleak_house.txt\n",
            "  Total Tokens: 432282\n",
            "  Unique Types: 20294\n",
            "\n",
            "File: david_copperfield.txt\n",
            "  Total Tokens: 446436\n",
            "  Unique Types: 17381\n",
            "\n",
            "File: dombey_and_son.txt\n",
            "  Total Tokens: 442635\n",
            "  Unique Types: 20277\n",
            "\n",
            "File: great_expectations.txt\n",
            "  Total Tokens: 228854\n",
            "  Unique Types: 13673\n",
            "\n",
            "File: hard_times.txt\n",
            "  Total Tokens: 131739\n",
            "  Unique Types: 11135\n",
            "\n",
            "File: little_dorrit.txt\n",
            "  Total Tokens: 416717\n",
            "  Unique Types: 18547\n",
            "\n",
            "File: nicholas_nickleby.txt\n",
            "  Total Tokens: 400495\n",
            "  Unique Types: 17975\n",
            "\n",
            "File: oliver_twist.txt\n",
            "  Total Tokens: 202156\n",
            "  Unique Types: 13108\n",
            "\n",
            "File: the_pickwick_papers.txt\n",
            "  Total Tokens: 377164\n",
            "  Unique Types: 19099\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Average Token and Type Calculation\n",
        "This section calculates the average number of tokens and types across all texts in the corpus.\n",
        "\n",
        "\n",
        "\n",
        "*   Average Tokens: The average number of words (including repetitions) per text.\n",
        "* Average Types: The average number of unique words (excluding repetitions) per text.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7i8MCmuYa5As"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Variables to store total tokens and types across all texts\n",
        "total_tokens_all = 0\n",
        "total_types_all = 0\n",
        "num_files = len(file_metrics)\n",
        "\n",
        "# Sum up the tokens and types from the previous results\n",
        "for metrics in file_metrics.values():\n",
        "    total_tokens_all += metrics['Total Tokens']\n",
        "    total_types_all += metrics['Unique Types']\n",
        "\n",
        "# Calculate averages\n",
        "average_tokens = total_tokens_all / num_files\n",
        "average_types = total_types_all / num_files\n",
        "\n",
        "\n",
        "print(f\"Average Number of Tokens: {average_tokens:.2f}\")\n",
        "print(f\"Average Number of Types: {average_types:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zxZ1wtWo7R3",
        "outputId": "c022789a-35e4-4b43-a694-614e5b481326"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Number of Tokens: 324831.50\n",
            "Average Number of Types: 16337.30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lexical Diversity Calcualtion\n",
        "This section calculates the lexical diversity for each text in the corpus and for the corpus as a whole.\n",
        "\n",
        "*   Lexical Diversity: The ratio of unique types (distinct words) to total tokens (all words) in a text. A higher value indicates a greater variety of vocabulary relative to the text's length.\n",
        "\n",
        "Key Outputs:\n",
        "\n",
        "\n",
        "1.   Per-Text Lexical Diversity:\n",
        "\n",
        "\n",
        "*   Shows the lexical diversity for each individual text, providing insight into how varied the vocabulary is in each novel.\n",
        "\n",
        "2.   Overall Lexical Diversity:\n",
        "\n",
        "*   Combines all texts in the corpus to compute the overall ratio of unique words to total words.\n",
        "\n",
        "\n",
        "The results highlight how Dickens' vocabulary varies across texts and how diverse his corpus is as a whole.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "D9I3USI4bhdL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate lexical diversity for each text\n",
        "for filename, metrics in file_metrics.items():\n",
        "    lexical_diversity = metrics['Unique Types'] / metrics['Total Tokens']\n",
        "    print(f\"File: {filename}\")\n",
        "    print(f\"  Lexical Diversity: {lexical_diversity:.4f}\")\n",
        "    print()\n",
        "\n",
        "# Calculate overall lexical diversity for the entire corpus\n",
        "overall_lexical_diversity = total_types_all / total_tokens_all\n",
        "print(f\"Overall Lexical Diversity of the Corpus: {overall_lexical_diversity:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQsuaD63o87c",
        "outputId": "69dff296-991a-4231-a474-f4c2bb6adbe4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File: a_tale_of_two_cities.txt\n",
            "  Lexical Diversity: 0.0700\n",
            "\n",
            "File: bleak_house.txt\n",
            "  Lexical Diversity: 0.0469\n",
            "\n",
            "File: david_copperfield.txt\n",
            "  Lexical Diversity: 0.0389\n",
            "\n",
            "File: dombey_and_son.txt\n",
            "  Lexical Diversity: 0.0458\n",
            "\n",
            "File: great_expectations.txt\n",
            "  Lexical Diversity: 0.0597\n",
            "\n",
            "File: hard_times.txt\n",
            "  Lexical Diversity: 0.0845\n",
            "\n",
            "File: little_dorrit.txt\n",
            "  Lexical Diversity: 0.0445\n",
            "\n",
            "File: nicholas_nickleby.txt\n",
            "  Lexical Diversity: 0.0449\n",
            "\n",
            "File: oliver_twist.txt\n",
            "  Lexical Diversity: 0.0648\n",
            "\n",
            "File: the_pickwick_papers.txt\n",
            "  Lexical Diversity: 0.0506\n",
            "\n",
            "Overall Lexical Diversity of the Corpus: 0.0503\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "14k1saV7nBIS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Frequency Distributions"
      ],
      "metadata": {
        "id": "WVL3U7KgrGZU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Most Frequent Words in the Corpus (Excluding Punctuation)\n",
        "This section identifies the top 10 most frequent words in the corpus after filtering out punctuation and non-alphabetic tokens.\n",
        "\n",
        "Steps:\n",
        "\n",
        "1.   Filter Tokens:\n",
        "\n",
        "\n",
        "\n",
        "*   Non-alphabetic tokens (e.g., punctuation, numbers) are excluded.\n",
        "*   Remaining tokens are converted to lowercase to ensure consistent counting.\n",
        "\n",
        "\n",
        "2.   Frequency Distribution:\n",
        "\n",
        "\n",
        "*   Calculates the frequency of each word in the filtered corpus.\n",
        "\n",
        "3. Display Top Words:\n",
        "\n",
        "\n",
        "*  Prints the 10 most frequent words along with their frequencies.\n",
        "\n",
        "Observations:\n",
        "\n",
        "*   Common words like \"the,\" \"and,\" and \"of\" dominate the corpus, reflecting their structural role in English\n",
        "\n",
        "* These words are likely stopwords, which may be removed in future steps to focus on thematic terms.\n"
      ],
      "metadata": {
        "id": "2d18TR4ncN9I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter out punctuation and non-alphabetic tokens from the entire corpus\n",
        "filtered_tokens = [word.lower() for word in tokens if word.isalpha()]\n",
        "\n",
        "# Calculate frequency distribution for the filtered corpus\n",
        "filtered_freq_dist = FreqDist(filtered_tokens)\n",
        "\n",
        "\n",
        "print(\"Most Frequent Words in the Corpus (Excluding Punctuation):\")\n",
        "most_common_filtered_words = filtered_freq_dist.most_common(10)  # Top 10 most frequent words\n",
        "for word, frequency in most_common_filtered_words:\n",
        "    print(f\"  {word}: {frequency}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-1-ygcvrKMy",
        "outputId": "0477acf8-2c18-49be-e5c7-e65dd398758c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most Frequent Words in the Corpus (Excluding Punctuation):\n",
            "  the: 18209\n",
            "  and: 9583\n",
            "  of: 8193\n",
            "  a: 7270\n",
            "  to: 7024\n",
            "  in: 4752\n",
            "  his: 4404\n",
            "  i: 3849\n",
            "  he: 3629\n",
            "  was: 3345\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Most Frequent Words in Each Text (Excluding Punctuation)\n",
        "\n",
        "This section calculates the most frequent words for each individual text in the corpus after excluding punctuation and non-alphabetic tokens.\n",
        "\n",
        "Steps:\n",
        "\n",
        "1. Read and Tokenzie Texts:\n",
        "\n",
        "\n",
        "*  Each text is read and tokenized into individual words.\n",
        "\n",
        "2. Filter Tokens:\n",
        "*  Non-alphabetic tokens are removed, and the remaining tokens are converted to lowercase for consistent counting.\n",
        "\n",
        "3. Frequency Distribution:\n",
        "\n",
        "\n",
        "*   The frequency of each word is calculated for every text.\n",
        "\n",
        "4. Display Results\n",
        "*   Prints the 10 most frequent words in each text along with their frequencies.\n",
        "\n",
        "Observations:\n",
        "\n",
        "\n",
        "*   Common function words like \"the,\" \"and,\" and \"of\" appear frequently across all texts, reflecting their essential role in sentence construction.\n",
        "*   The frequency distribution helps identify repetitive patterns within individual texts, which can later be refined by removing stopwords to highlight thematic vocabulary.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_WZF_Y0fdkr1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Frequency distribution for each individual text, excluding punctuation\n",
        "for filename in file_metrics:\n",
        "    file_path = os.path.join(folder_path, filename)\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        text = file.read()\n",
        "        text_tokens = nltk.word_tokenize(text)\n",
        "\n",
        "        # Filter out punctuation and non-alphabetic tokens\n",
        "        text_filtered_tokens = [word.lower() for word in text_tokens if word.isalpha()]\n",
        "        text_filtered_freq_dist = FreqDist(text_filtered_tokens)\n",
        "\n",
        "\n",
        "        print(f\"\\nMost Frequent Words in {filename} (Excluding Punctuation):\")\n",
        "        for word, frequency in text_filtered_freq_dist.most_common(10):  # Top 10\n",
        "            print(f\"  {word}: {frequency}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sNjJP9b2_CHE",
        "outputId": "1fa25ad5-5d42-4fcf-b49a-47068646d1a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Most Frequent Words in a_tale_of_two_cities.txt (Excluding Punctuation):\n",
            "  the: 8199\n",
            "  and: 5044\n",
            "  of: 4122\n",
            "  to: 3555\n",
            "  a: 2995\n",
            "  in: 2645\n",
            "  it: 2039\n",
            "  his: 2007\n",
            "  i: 1962\n",
            "  that: 1940\n",
            "\n",
            "Most Frequent Words in bleak_house.txt (Excluding Punctuation):\n",
            "  the: 15077\n",
            "  and: 12568\n",
            "  to: 10099\n",
            "  i: 9315\n",
            "  of: 8601\n",
            "  a: 7600\n",
            "  in: 6268\n",
            "  it: 5272\n",
            "  that: 4982\n",
            "  he: 4622\n",
            "\n",
            "Most Frequent Words in david_copperfield.txt (Excluding Punctuation):\n",
            "  the: 13869\n",
            "  i: 13449\n",
            "  and: 12337\n",
            "  to: 10535\n",
            "  of: 8796\n",
            "  a: 7935\n",
            "  in: 6272\n",
            "  that: 5377\n",
            "  was: 5310\n",
            "  my: 5204\n",
            "\n",
            "Most Frequent Words in dombey_and_son.txt (Excluding Punctuation):\n",
            "  the: 17124\n",
            "  and: 13815\n",
            "  to: 9436\n",
            "  of: 9308\n",
            "  a: 7288\n",
            "  in: 6429\n",
            "  his: 5150\n",
            "  that: 5148\n",
            "  her: 4736\n",
            "  i: 4713\n",
            "\n",
            "Most Frequent Words in great_expectations.txt (Excluding Punctuation):\n",
            "  the: 8296\n",
            "  and: 7020\n",
            "  i: 6548\n",
            "  to: 5132\n",
            "  of: 4538\n",
            "  a: 4036\n",
            "  in: 3047\n",
            "  that: 3037\n",
            "  was: 2811\n",
            "  it: 2737\n",
            "\n",
            "Most Frequent Words in hard_times.txt (Excluding Punctuation):\n",
            "  the: 4459\n",
            "  and: 3346\n",
            "  to: 2983\n",
            "  of: 2682\n",
            "  a: 2301\n",
            "  i: 2160\n",
            "  in: 1823\n",
            "  you: 1569\n",
            "  it: 1386\n",
            "  he: 1374\n",
            "\n",
            "Most Frequent Words in little_dorrit.txt (Excluding Punctuation):\n",
            "  the: 15872\n",
            "  and: 11162\n",
            "  to: 9905\n",
            "  of: 9402\n",
            "  a: 7244\n",
            "  i: 5957\n",
            "  in: 5925\n",
            "  that: 5260\n",
            "  it: 5176\n",
            "  he: 4789\n",
            "\n",
            "Most Frequent Words in nicholas_nickleby.txt (Excluding Punctuation):\n",
            "  the: 14963\n",
            "  and: 11492\n",
            "  to: 8395\n",
            "  of: 8366\n",
            "  a: 7450\n",
            "  in: 5118\n",
            "  i: 5047\n",
            "  his: 4371\n",
            "  that: 4259\n",
            "  he: 3950\n",
            "\n",
            "Most Frequent Words in oliver_twist.txt (Excluding Punctuation):\n",
            "  the: 9727\n",
            "  and: 5384\n",
            "  of: 3944\n",
            "  to: 3922\n",
            "  a: 3754\n",
            "  he: 2487\n",
            "  in: 2415\n",
            "  his: 2341\n",
            "  that: 1934\n",
            "  it: 1834\n",
            "\n",
            "Most Frequent Words in the_pickwick_papers.txt (Excluding Punctuation):\n",
            "  the: 18209\n",
            "  and: 9583\n",
            "  of: 8193\n",
            "  a: 7270\n",
            "  to: 7024\n",
            "  in: 4752\n",
            "  his: 4404\n",
            "  i: 3849\n",
            "  he: 3629\n",
            "  was: 3345\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Very Infrequent Words in the Entire Corpus\n",
        "This section identifies words that appear only once in the entire corpus.\n",
        "\n",
        "Steps:\n",
        "1. Filter ingrequent words:\n",
        "\n",
        "*   Extract words from the frequency distribution with a frequency of 1.\n",
        "\n",
        "2. Display Result\n",
        "\n",
        "*   Show the total number of infrequent words and a sample of 20 such words.\n",
        "\n",
        "Observations:\n",
        "\n",
        "*  The corpus contains 5,525 very infrequent words, many of which may reflect unique contexts, proper nouns, or specialized vocabulary.\n",
        "\n",
        "\n",
        "\n",
        "* Include words like \"illumines,\" \"obscurity,\" and \"authentication,\" which add richness to the text but appear rarely.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kJ7CdpPUBeZP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find infrequent words in the entire corpus\n",
        "infrequent_words = [word for word, freq in filtered_freq_dist.items() if freq == 1]\n",
        "\n",
        "print(f\"Number of Infrequent Words in the Corpus: {len(infrequent_words)}\")\n",
        "print(\"Sample of Very Infrequent Words (Corpus-Level):\")\n",
        "print(infrequent_words[:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ckCaBKRVBij7",
        "outputId": "3493b2e9-fd77-46ac-c0b8-00b67a7cd922"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Infrequent Words in the Corpus: 5525\n",
            "Sample of Very Infrequent Words (Corpus-Level):\n",
            "['illumines', 'obscurity', 'discrimination', 'smiggers', 'accrue', 'hornsey', 'highgate', 'enlarging', 'advancement', 'emanating', 'sanction', 'nominated', 'authenticated', 'investigations', 'recognises', 'defraying', 'postage', 'deliberated', 'considers', 'emanated']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Very Infrequent Words in Individual Texts\n",
        "This section identifies words that appear only once in each individual text.\n",
        "\n",
        "**Steps:**\n",
        "1. Process Each Text:\n",
        "\n",
        "*   For each text, calculate the frequency distribution after filtering out punctuation and non-alphabetic tokens.\n",
        "\n",
        "2. Identify infrequent Words:\n",
        "*   Extract words with a frequency of 1 in each text.\n",
        "\n",
        "3. Display Results\n",
        "\n",
        "*   Show the total number of infrequent words for each text and a sample of 20 such words.\n",
        "\n",
        "**Observations**:\n",
        "\n",
        "Each text has thousands of very infrequent words. For example:\n",
        "\n",
        "*  A Tale of Two Cities: 4,263 infrequent words (e.g., \"foolishness,\" \"incredulity\").\n",
        "\n",
        "\n",
        "*   Bleak House: 5,563 infrequent words (e.g., \"blemish,\" \"parsimony\").\n",
        "\n",
        "*  These words contribute to the unique flavor and depth of each text, often highlighting rare descriptions or names.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4_dSCoOoBxg7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find infrequent words in each text\n",
        "for filename in file_metrics:\n",
        "    file_path = os.path.join(folder_path, filename)\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        text = file.read()\n",
        "        text_tokens = nltk.word_tokenize(text)\n",
        "\n",
        "        # Filter out punctuation and non-alphabetic tokens\n",
        "        text_filtered_tokens = [word.lower() for word in text_tokens if word.isalpha()]\n",
        "        text_filtered_freq_dist = FreqDist(text_filtered_tokens)\n",
        "\n",
        "        # Find words that occur only once\n",
        "        infrequent_words = [word for word, freq in text_filtered_freq_dist.items() if freq == 1]\n",
        "\n",
        "        print(f\"\\nNumber of Infrequent Words in {filename}: {len(infrequent_words)}\")\n",
        "        print(f\"Sample of Very Infrequent Words in {filename}:\")\n",
        "        print(infrequent_words[:20])  # Display a sample of 20 very infrequent words\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mCkNSA4ZB4oj",
        "outputId": "bbc23735-d9dd-42b0-b04d-cb609f9db84b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of Infrequent Words in a_tale_of_two_cities.txt: 4263\n",
            "Sample of Very Infrequent Words in a_tale_of_two_cities.txt:\n",
            "['foolishness', 'incredulity', 'noisiest', 'superlative', 'clearer', 'crystal', 'preserves', 'fishes', 'revelations', 'conceded', 'southcott', 'attained', 'announcing', 'rapping', 'supernaturally', 'deficient', 'originality', 'congress', 'chickens', 'shield']\n",
            "\n",
            "Number of Infrequent Words in bleak_house.txt: 5645\n",
            "Sample of Very Infrequent Words in bleak_house.txt:\n",
            "['blemish', 'exaggerated', 'parsimony', 'enlarging', 'inserted', 'quotation', 'shakespeare', 'sonnets', 'dyer', 'august', 'lewes', 'wilfully', 'mislead', 'cesenate', 'investigated', 'giuseppe', 'republished', 'rheims', 'historian', 'renowned']\n",
            "\n",
            "Number of Infrequent Words in david_copperfield.txt: 5216\n",
            "Sample of Very Infrequent Words in david_copperfield.txt:\n",
            "['dickens', 'avowals', 'simultaneously', 'gifts', 'inevitably', 'infants', 'verified', 'advertised', 'jackets', 'cash', 'guaranteed', 'raffle', 'winner', 'proudest', 'meander', 'scotland', 'posthumous', 'magnate', 'overcame', 'adage']\n",
            "\n",
            "Number of Infrequent Words in dombey_and_son.txt: 5535\n",
            "Sample of Very Infrequent Words in dombey_and_son.txt:\n",
            "['analogous', 'essential', 'prepossessing', 'spotty', 'striding', 'notching', 'scythe', 'phosphorescently', 'squaring', 'inhaling', 'fictitious', 'autograph', 'rivers', 'rainbows', 'planets', 'circled', 'orbits', 'abbreviations', 'meanings', 'domini']\n",
            "\n",
            "Number of Infrequent Words in great_expectations.txt: 4552\n",
            "Sample of Very Infrequent Words in great_expectations.txt:\n",
            "['explicit', 'photographs', 'inscription', 'freckled', 'sickly', 'lozenges', 'brothers', 'universal', 'indebted', 'religiously', 'alexander', 'abraham', 'tobias', 'roger', 'aforesaid', 'intersected', 'feeding', 'lair', 'soaked', 'smothered']\n",
            "\n",
            "Number of Infrequent Words in hard_times.txt: 4206\n",
            "Sample of Very Infrequent Words in hard_times.txt:\n",
            "['teach', 'plant', 'root', 'emphasized', 'underscoring', 'commodious', 'caves', 'bristled', 'skirts', 'bald', 'plantation', 'firs', 'knobs', 'plum', 'neckcloth', 'unaccommodating', 'stubborn', 'imperial', 'gallons', 'murdering']\n",
            "\n",
            "Number of Infrequent Words in little_dorrit.txt: 5618\n",
            "Sample of Very Infrequent Words in little_dorrit.txt:\n",
            "['demerits', 'publication', 'russian', 'irish', 'mitigation', 'climax', 'directors', 'metamorphosed', 'biographer', 'supernaturally', 'joe', 'whosoever', 'rarity', 'tracts', 'arid', 'verdure', 'demarcation', 'blistered', 'hindoos', 'russians']\n",
            "\n",
            "Number of Infrequent Words in nicholas_nickleby.txt: 5176\n",
            "Sample of Very Infrequent Words in nicholas_nickleby.txt:\n",
            "['notable', 'unfitness', 'chemist', 'crafts', 'impostors', 'lowest', 'imbecility', 'entrusted', 'cornerstone', 'structure', 'absurdity', 'practitioner', 'heal', 'pettifoggers', 'tense', 'facilities', 'attainment', 'rochester', 'partridge', 'strap']\n",
            "\n",
            "Number of Infrequent Words in oliver_twist.txt: 4163\n",
            "Sample of Very Infrequent Words in oliver_twist.txt:\n",
            "['refrain', 'fictitious', 'anciently', 'towns', 'wit', 'mortality', 'prefixed', 'memoirs', 'inestimable', 'concise', 'biography', 'extant', 'literature', 'enviable', 'befall', 'inducing', 'respiration', 'unequally', 'poised', 'grandmothers']\n",
            "\n",
            "Number of Infrequent Words in the_pickwick_papers.txt: 5525\n",
            "Sample of Very Infrequent Words in the_pickwick_papers.txt:\n",
            "['illumines', 'obscurity', 'discrimination', 'smiggers', 'accrue', 'hornsey', 'highgate', 'enlarging', 'advancement', 'emanating', 'sanction', 'nominated', 'authenticated', 'investigations', 'recognises', 'defraying', 'postage', 'deliberated', 'considers', 'emanated']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "2HvOv0icnEaY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Impact of Preprocessing Techniques\n",
        "\n"
      ],
      "metadata": {
        "id": "9HoaTdByD_Iq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Removing Stop Words\n",
        "\n",
        "Stop words, such as \"the,\" \"and,\" \"of,\" and \"to,\" are frequently used but do not add meaningful context to a text. By removing these words, the corpus focuses more on thematic or content-related vocabulary\n",
        "\n",
        "Effect on the Corpus:\n",
        "\n",
        "\n",
        "*  Reduces the overall token count significantly.\n",
        "\n",
        "*  Reveals more meaningful words (e.g., character names, thematic terms) in the frequency distribution.\n",
        "\n",
        "*   Helps in tasks like topic modeling or thematic analysis by removing noise.\n",
        "\n",
        "\n",
        "\n",
        "*italicized text*\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "IgvIBg1fkTjC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Get the list of English stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Filter tokens to exclude stop words\n",
        "filtered_tokens_no_stopwords = [word for word in filtered_tokens if word not in stop_words]\n",
        "\n",
        "# Recalculate frequency distribution\n",
        "freq_dist_no_stopwords = FreqDist(filtered_tokens_no_stopwords)\n",
        "\n",
        "print(\"Most Frequent Words After Removing Stop Words:\")\n",
        "for word, frequency in freq_dist_no_stopwords.most_common(10):\n",
        "    print(f\"{word}: {frequency}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5y0G678DETYF",
        "outputId": "3b1d1cec-26b0-41ee-d1e6-9b250da425d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most Frequent Words After Removing Stop Words:\n",
            "said: 3315\n",
            "pickwick: 2332\n",
            "sir: 1493\n",
            "sam: 1166\n",
            "replied: 1003\n",
            "man: 979\n",
            "weller: 966\n",
            "old: 888\n",
            "one: 739\n",
            "gentleman: 711\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Removing Words Below a Minimum Frequency\n",
        "\n",
        "Low-frequency words are often unique terms, typos, or proper nouns that might not contribute significantly to broader patterns in the text. Removing these reduces noise and focuses analysis on more prominent vocabulary.\n",
        "\n",
        "Effect on the Corpus:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*   Eliminates thousands of very rare words, reducing corpus size.\n",
        "\n",
        "*  Focuses on commonly used words that reflect the text's overall themes and style.\n",
        "\n",
        "\n",
        "*   Helps simplify models that rely on reduced vocabulary, such as word embeddings.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-xYlZDBWF9O-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a minimum frequency threshold\n",
        "min_frequency = 5\n",
        "\n",
        "# Filter tokens that occur at least min_frequency times\n",
        "filtered_tokens_min_freq = [word for word in filtered_tokens if filtered_freq_dist[word] >= min_frequency]\n",
        "\n",
        "# Recalculate frequency distribution\n",
        "freq_dist_min_freq = FreqDist(filtered_tokens_min_freq)\n",
        "\n",
        "\n",
        "print(f\"Most Frequent Words After Removing Words Below {min_frequency} Occurrences:\")\n",
        "for word, frequency in freq_dist_min_freq.most_common(10):\n",
        "    print(f\"{word}: {frequency}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RqAANsFHGRNa",
        "outputId": "ec654fd8-ae85-4f82-b270-30c91a9040b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most Frequent Words After Removing Words Below 5 Occurrences:\n",
            "the: 18209\n",
            "and: 9583\n",
            "of: 8193\n",
            "a: 7270\n",
            "to: 7024\n",
            "in: 4752\n",
            "his: 4404\n",
            "i: 3849\n",
            "he: 3629\n",
            "was: 3345\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **\"So What?\" Interpretation:**\n",
        "\n",
        "1. **Removing Stop Words:**\n",
        "\n",
        "*  **Impact on Themes and Content:** By removing stop words, the corpus becomes\n",
        "more focused on words with semantic importance. This shift helps highlight thematic vocabulary such as \"justice,\" \"poverty,\" or \"family,\" which are central to Dickens' works.\n",
        "\n",
        "*  **Application:** This preprocessing step is particularly valuable for tasks like topic modeling, where stopwords could dominate and obscure meaningful clusters.\n",
        "\n",
        "*  **Potential Trade-Off:** Removing stop words might also exclude rare but meaningful function words (e.g., \"not\" or \"never\") that carry critical context in specific passages.\n",
        "\n",
        "2. **Removing Low-Frequency Words:**\n",
        "\n",
        "* **Impact on Simplification:** Filtering out rare words significantly reduces the size of the vocabulary, making subsequent analysis (e.g., word embeddings or clustering) computationally efficient.\n",
        "\n",
        "* **Focus on Core Vocabulary:** This allows analysts to concentrate on frequently used words that reflect the broader themes of the text, rather than outliers or typographical errors.\n",
        "\n",
        "* **Trade-Off:** While this improves efficiency, rare words like character names or unique descriptors are lost, which might diminish the richness of certain analyses.\n",
        "\n",
        "3. **Broader Effects on Understanding the Corpus**\n",
        "\n",
        "* **Quantitative Impact:** Both techniques reduce the token count and alter the frequency distribution, shifting focus from structural or rare terms to meaningful thematic words.\n",
        "\n",
        "* **Qualitative Impact:** These changes can highlight key terms central to Dickens' storytelling, aiding literary analysis, thematic exploration, and historical language studies.\n",
        "\n",
        "4. **Context-Specific Decisions:**\n",
        "\n",
        "  The decision to apply these techniques depends on the goal:\n",
        "\n",
        "* For sentiment analysis, removing stop words and rare words is ideal to emphasize sentiment-laden terms.\n",
        "\n",
        "* For character studies or unique descriptors, retaining low-frequency words becomes more critical.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-fZm760UGczD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Exploring the Use of a Specific Word Across Texts Using .concordance**\n",
        "\n",
        "This analysis investigates how a particular word (e.g., \"justice\", \"love\", \"death\") is used across the texts in the corpus. By leveraging the .concordance function, we explore the context in which the word appears, revealing thematic patterns and insights into its significance in the works of Charles Dickens.\n"
      ],
      "metadata": {
        "id": "jQhZeOEzItTS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.text import Text\n",
        "\n",
        "# Word to analyze\n",
        "word_to_analyze = \"justice\"\n",
        "\n",
        "# Analyze the context of the word across each text\n",
        "for filename in file_metrics:\n",
        "    file_path = os.path.join(folder_path, filename)\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        text = file.read()\n",
        "        text_tokens = nltk.word_tokenize(text)\n",
        "        filtered_text_tokens = [word.lower() for word in text_tokens if word.isalpha()]\n",
        "\n",
        "        nltk_text = Text(filtered_text_tokens)\n",
        "\n",
        "        print(f\"\\nContext for the word '{word_to_analyze}' in {filename}:\")\n",
        "        nltk_text.concordance(word_to_analyze, width=80, lines=10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wVurpzIoLVan",
        "outputId": "97c35625-6f99-4c5e-ba68-c86aa12e1737"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Context for the word 'justice' in a_tale_of_two_cities.txt:\n",
            "Displaying 8 of 8 matches:\n",
            "ight from the dock at my lord chief justice himself and pulled him off the benc\n",
            "owards the visage of the lord chief justice in the court of king s bench the fl\n",
            "rough the chambers the tribunals of justice and all society except the scarecro\n",
            "ther rejoined i believe it i do you justice i believe his constraint was so man\n",
            " at paris for the love of heaven of justice of generosity of the honour of your\n",
            " prisoner in danger of death to his justice honour and good name his resolution\n",
            "s journey for the love of heaven of justice of generosity of the honour of your\n",
            "e in a state of mind to impeach the justice of the republic she will be full of\n",
            "\n",
            "Context for the word 'justice' in bleak_house.txt:\n",
            "Displaying 10 of 28 matches:\n",
            "ould involve an occasional delay of justice and a trifling amount of confusion \n",
            "es a great deal of thought chancery justice is so difficult to who s this miss \n",
            "that interest accompanies to do him justice he is exceedingly unwilling to trou\n",
            "ow we can t take that in a court of justice gentlemen it s terrible depravity p\n",
            "y mind and by angrily demanding the justice i never get that i am able to keep \n",
            "the face to tell me i have received justice and therefore am dismissed my lord \n",
            "ould have their rights according to justice that s what you to be sure returns \n",
            "ns snagsby no you don t do yourself justice it an t what you endeavour to do sa\n",
            "ablished if there were any sense or justice in the court of oh what a great if \n",
            " summoned by quebec and malta to do justice to the pork and greens over which b\n",
            "\n",
            "Context for the word 'justice' in david_copperfield.txt:\n",
            "Displaying 10 of 25 matches:\n",
            " in that formal parade of executing justice and when we got there suddenly twis\n",
            "by the transaction let me do myself justice however i was moved by no intereste\n",
            " as if it were a prison or a bar of justice this position i continued to occupy\n",
            "r i considered it an act of greater justice to myself and perhaps of more respe\n",
            " have some compensation if there be justice in the world my passion takes away \n",
            "etables to all of which i did ample justice and which were all excellent but my\n",
            "lied when you come to do steerforth justice and to like him as well as i not un\n",
            " me that what micawber has to do in justice to himself in justice to his family\n",
            " has to do in justice to himself in justice to his family and i will even go so\n",
            " i will even go so far as to say in justice to society by which he has been hit\n",
            "\n",
            "Context for the word 'justice' in dombey_and_son.txt:\n",
            "Displaying 10 of 13 matches:\n",
            "evice returned louisa but do me the justice to remember my dear louisa said mis\n",
            "rned his sister you do miss tox but justice as a man of your penetration was su\n",
            "t upon your trial before a court of justice at present louisa returned mr dombe\n",
            "r bagstock slightly i should not do justice to the friendship which exists betw\n",
            "n object of such strong james do me justice said his brother i have claimed not\n",
            "ty him it was the more questionable justice paul thought in the doctor from his\n",
            "t still i m not a to do miss nipper justice she spoke more for her young mistre\n",
            "wards it gloomy as it was and rigid justice as she rendered to its gloom she fo\n",
            " i am much obliged to you you do me justice i assure you you were going to say \n",
            " just to a very humble claimant for justice at her mere dependant of mr dombey \n",
            "\n",
            "Context for the word 'justice' in great_expectations.txt:\n",
            "Displaying 10 of 10 matches:\n",
            "all their doo and maintaining equal justice betwixt man and man my father were \n",
            "d their height whether myrmidons of justice especially sent down from london wo\n",
            "rty and partially drunk minister of justice asked me if i would like to step in\n",
            "mmand a full view of the lord chief justice in his wig and robes that awful per\n",
            "ondon the more so as the lord chief justice s proprietor wore from his hat down\n",
            "excuse and each of us did the other justice nor did i ever regard him as having\n",
            "e representative of british law and justice in that chair that day chapter xxv \n",
            "r reluctant concession to truth and justice if i wanted to deny it i should thi\n",
            "ving been stolen from some court of justice and perhaps his knowledge of its an\n",
            "ucceeded in evading the officers of justice but being at length seized while in\n",
            "\n",
            "Context for the word 'justice' in hard_times.txt:\n",
            "Displaying 6 of 6 matches:\n",
            "nd you do i am happy to say so much justice to the education you have received \n",
            "ee the application of the to do him justice he did not at all she passed it awa\n",
            " he said it earnestly and to do him justice he had in gauging fathomless deeps \n",
            "re enrolled upon the holy scroll of justice and of union is appropriately capab\n",
            "ed a little suspicious in courts of justice i believe when an innocent happens \n",
            "be found how is he to be saved from justice in the few hours that i can possibl\n",
            "\n",
            "Context for the word 'justice' in little_dorrit.txt:\n",
            "Displaying 10 of 12 matches:\n",
            "on i wither away in mine inexorable justice is done what do i owe on this score\n",
            "ey could scarcely have done greater justice to their education in the midst of \n",
            " world knows hah and yet he escaped justice monsieur the law could not prove it\n",
            " consider it worth my while to have justice done to arthur s father hey it does\n",
            " with an unsatisfied claim upon his justice returned and that alone was a subje\n",
            "making this observation forcibly in justice to myself that you ought to have go\n",
            "int which i could scarcely show the justice of more emphatically than by having\n",
            "g traveller complimented her on the justice of the distinction it was what he h\n",
            "ner to whom no painter could render justice but for want of a form of words in \n",
            "een doing the attempt to get public justice done to you and unless i have some \n",
            "\n",
            "Context for the word 'justice' in nicholas_nickleby.txt:\n",
            "Displaying 10 of 21 matches:\n",
            "of and indeed to do these gentlemen justice many of them are to this day in the\n",
            " occupation on crutches fountain of justice were these things to last this was \n",
            "o speedily proceeded to do it ample justice nicholas drew up his chair but his \n",
            " the force of this reasoning or the justice of this conclusion but without trou\n",
            "ing my native land i admit the full justice of the remark i am proud of this fr\n",
            "bout us at that time will do me the justice to own that if i said that once i s\n",
            "ver replied kate and to do nickleby justice she never had lost and to do marrie\n",
            " and to do married ladies as a body justice they seldom do lose any occasion of\n",
            " were remarkable for doing it ample justice in which respect messrs pyke and pl\n",
            " i had the painting of her i do her justice so feeling quite satisfied that she\n",
            "\n",
            "Context for the word 'justice' in oliver_twist.txt:\n",
            "Displaying 10 of 19 matches:\n",
            "heavy accusation of which to do her justice she was wholly innocent in thought \n",
            "he limits of this chapter to do him justice he was as far as his power was not \n",
            "ecimen of his mode of administering justice the offence had been committed with\n",
            "urt into this dispensary of summary justice by the back way it was a small pave\n",
            "would deal as leniently with him as justice would allow he has been hurt alread\n",
            " them that it was possible even for justice itself to confound the innocent wit\n",
            " decoration of his person to do him justice this was by no means an habitual we\n",
            "strong confirmatory evidence of the justice of the jew s supposition and when a\n",
            "m chargeable to the parish ought in justice to be visited with no punishment at\n",
            "chitling in whose conduct it is but justice to say there was nothing very consp\n",
            "\n",
            "Context for the word 'justice' in the_pickwick_papers.txt:\n",
            "Displaying 10 of 36 matches:\n",
            "im he swindled me i wo bear it have justice pickwick i wo stand it and with sun\n",
            "ucceeded to which we can no more do justice than the mayor could although he is\n",
            "ear leo hunter repeat it she can do justice to it sir she will repeat it in cha\n",
            "t accoutred as a russian officer of justice with a tremendous knout in his hand\n",
            "ride at all so let us fall to ample justice was done to the meal and when they \n",
            "n to induce the party to yield full justice to the meal and as little pressing \n",
            " of a single combat with the to the justice cried a dozen voices run avay said \n",
            "mpliments veller compliments to the justice and tell him i spiled his beadle an\n",
            "nded and guarded by the officers of justice like a common criminal grummer in t\n",
            "ed and secured and it is but common justice both to him and winkle to say that \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interpretation of Results**\n",
        "\n",
        "Patterns Across Texts:\n",
        "Thematic Use:\n",
        "\n",
        "Words like \"justice\" often align with key themes in Dickens works, such as social inequality and legal systems.\n",
        "\n",
        "Concordance lines reveal how the term is woven into characters' dialogues, narrations, or reflections.\n",
        "\n",
        "Contextual Variations:\n",
        "\n",
        "The use of the word might differ between texts. For example:\n",
        "\n",
        "A Tale of Two Cities might use \"justice\" in the context of revolutionary fervor.\n",
        "\n",
        "Great Expectations might focus on personal or moral interpretations of justice.\n",
        "\n",
        "Collocations:\n",
        "\n",
        "Examining surrounding words in the concordance lines can highlight significant collocations (e.g., \"scales of justice\", \"justice and equality\"). These pairings often enhance our understanding of how the word is framed.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vTI0lPduMZZs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**\n",
        "\n",
        "The analysis of the corpus of Charles Dickens' works reveals valuable insights into his use of language and the thematic richness of his writing. The examination of type/token ratios highlights an expected pattern: longer texts exhibit lower lexical diversity due to increased repetition of common words. This trend aligns with the nature of narrative storytelling, where certain words (e.g., articles, conjunctions, or thematic terms) are frequently reused. At the same time, shorter texts maintain higher diversity, indicating less repetition and possibly more concentrated linguistic variety.\n",
        "\n",
        "The frequency distribution of words further deepens our understanding of Dickens' corpus. Stop words like \"the,\" \"and,\" and \"of\" dominate the word frequency rankings, as expected, given their structural role in the English language. However, removing these stop words uncovered more meaningful patterns, including the prominence of thematic terms and character names. For instance, key words such as \"justice,\" \"love,\" and \"family\" appear frequently and are central to understanding the themes Dickens explores in his novels.\n",
        "\n",
        "The unique analysis of the word \"justice\" across texts using the .concordance function provided an intriguing look at its contextual usage. In A Tale of Two Cities, \"justice\" is tied to revolutionary ideals, while in Great Expectations, it reflects personal struggles and moral dilemmas. This contextual exploration highlights how Dickens' language adapts to the themes and settings of his novels, enriching the reader's understanding of his storytelling.\n",
        "\n",
        "Despite these findings, certain aspects remain unexplored. For example, examining how Dickens' language evolves over time or comparing his works to other authors of the same period could provide broader insights. Additionally, incorporating texts from different genres or categories (e.g., non-fiction or letters) into the corpus could reveal how his style varies across different contexts.\n",
        "\n",
        "This corpus and analysis offer valuable tools for understanding Dickens' contributions to English literature and language. By exploring lexical diversity, word frequency, and contextual usage, I gain a deeper appreciation of his linguistic artistry. For future research, integrating advanced natural language processing techniques, such as sentiment analysis or topic modeling, could further illuminate the nuances of his work and its impact on readers."
      ],
      "metadata": {
        "id": "3YHsqPBSNfBo"
      }
    }
  ]
}