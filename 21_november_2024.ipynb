{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## NLTK corpora!\n",
        "\n",
        "- what they are!\n",
        "- what they can do!\n",
        "- how you can build one!\n",
        "- how to use .concordance()\n",
        "\n",
        "Let's quickly practice using just ONE corpus - `state_union`\n",
        "\n",
        "This is some practice for Assignment 1!"
      ],
      "metadata": {
        "id": "DfhsdiKBjSmJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_o0wpJsOjQl0"
      },
      "outputs": [],
      "source": [
        "# get the stuff we need\n",
        "import nltk\n",
        "nltk.download('state_union')\n",
        "from nltk.corpus import state_union"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# what even is this thing\n",
        "state_union"
      ],
      "metadata": {
        "id": "cVgMnu2rkVlO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fileids is the metadata/fileid for each text in the corpus\n",
        "state_union.fileids()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ToaFrFuvkZBf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# they are just another list of strings...\n",
        "for id in state_union.fileids():\n",
        "  if id[0] == '2':\n",
        "    print(id)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "qv1v_3YFsf-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "state_union.words('1963-Johnson.txt')"
      ],
      "metadata": {
        "collapsed": true,
        "id": "YJTuOTSOFuEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# can pick any one text or a selection of texts:\n",
        "gwb_2001 = state_union.words(['2001-GWBush-1.txt','2001-GWBush-2.txt'])"
      ],
      "metadata": {
        "id": "qtUzShIPkteX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# and now we can start building a profile!\n",
        "gwb_fd_raw = nltk.FreqDist([token.lower() for token in gwb_2001])\n",
        "gwb_fd_cleaned = nltk.FreqDist([token.lower() for token in gwb_2001 if token.isalpha() and len(token) > 5])"
      ],
      "metadata": {
        "id": "VDeZSa90lOep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compare and contrast the effects of preprocssing on our output:"
      ],
      "metadata": {
        "id": "7Lm5o62itOoj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gwb_fd_raw.most_common(20)"
      ],
      "metadata": {
        "id": "6KdEyyTYmHin",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gwb_fd_cleaned.most_common(20)"
      ],
      "metadata": {
        "id": "E33K4N8TtKux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How to make concordances?\n",
        "\n",
        "Need to wrap the words in a different function - the `Text` class in NLTK\n",
        "\n"
      ],
      "metadata": {
        "id": "g47sI-DhmeXc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a special Text version of the corpus\n",
        "from nltk.text import Text\n",
        "# remember, gwb_2001 is the tokens of the two 2001 speeches\n",
        "gwb_2001_corpus = Text(gwb_2001)"
      ],
      "metadata": {
        "id": "rDZmZan9muuX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# and now free to find concordances - the original corpus remains intact\n",
        "gwb_2001_corpus.concordance('applause', width = 20, lines = 200)"
      ],
      "metadata": {
        "id": "cIU_5GeLm3vj",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Assignment one** asks you to profile 10 texts in your corpus. At this point, you can use a loop to gather this information and print it to the console. Let's say you want to compare all the GWB texts in `state_union`. We could start with a list of the files we like..."
      ],
      "metadata": {
        "id": "9pWyy9WLn4nr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_files = ['2001-GWBush-1.txt',\n",
        " '2001-GWBush-2.txt',\n",
        " '2002-GWBush.txt',\n",
        " '2003-GWBush.txt',\n",
        " '2004-GWBush.txt',\n",
        " '2005-GWBush.txt',\n",
        " '2006-GWBush.txt']"
      ],
      "metadata": {
        "id": "kDrUn58On6Te"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fd2001a = nltk.FreqDist(state_union.words('2001-GWBush-1.txt'))\n",
        "fd2001b = nltk.FreqDist(state_union.words('2001-GWBush-2.txt'))"
      ],
      "metadata": {
        "id": "iw8nS6MxKC9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "for entry in fd2001a:\n",
        "  if re.match('terr', entry):\n",
        "    print(entry)\n",
        "    print(fd2001a[entry])\n"
      ],
      "metadata": {
        "id": "UBBagYm9KS1u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for entry in fd2001b:\n",
        "  if re.match('terr', entry):\n",
        "    print(entry)\n",
        "    print(fd2001b[entry])"
      ],
      "metadata": {
        "id": "jJW0aw1fK3em"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tally = 0\n",
        "\n",
        "for entry in fd2001b:\n",
        "  if entry.startswith('terror'):\n",
        "    print(entry)\n",
        "    print(fd2001b[entry])\n",
        "    tally = tally + fd2001b[entry]\n",
        "\n",
        "print(tally/fd2001b.total() * 100)"
      ],
      "metadata": {
        "id": "PXwCzV3vLZy_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fd2001b.total()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "kxTh1fMyMKb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fd2001b['terrorists']"
      ],
      "metadata": {
        "id": "hW3I0bnUKXd8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then loop through them, asking for information about each text.\n",
        "\n",
        "Here we get the number of words:"
      ],
      "metadata": {
        "id": "6ccPn3uwoT5Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# count all the words in our texts?\n",
        "for file in my_files:\n",
        "  print(file)\n",
        "  # note that we pass the name of the file to the original corpus\n",
        "  print(len(state_union.words(file)))"
      ],
      "metadata": {
        "id": "1i_hGLf8n-Zo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How can we explore the top n more frequent words?"
      ],
      "metadata": {
        "id": "qM9uRnyConR0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for file in my_files:\n",
        "  # this fd will be recreated each loop - note the heavy cleaning i'm doing\n",
        "  fd = nltk.FreqDist([token.lower() for token in state_union.words(file) if token.isalpha() and len(token) > 5])\n",
        "  print(file)\n",
        "  print(fd.most_common(5))"
      ],
      "metadata": {
        "id": "eGxqhcjFok6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And what about hapaxes? Note that you will get very different results if you do / do not use `.lower()`"
      ],
      "metadata": {
        "id": "SWlStZXBpqUW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for file in my_files:\n",
        "  fd = nltk.FreqDist([token.lower() for token in state_union.words(file) if token.isalpha()])\n",
        "  print(file)\n",
        "  print(fd.hapaxes())"
      ],
      "metadata": {
        "id": "5OI_bAqJppGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating our own corpus...\n",
        "\n",
        "- need your own .txt files\n",
        "- and a way to get them into Colab / Python\n",
        "- notebook has examples, a bit messy to try in class"
      ],
      "metadata": {
        "id": "oSW2xzjeqdMz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create 3 \"texts\"\n",
        "text1 = 'Kinda like a cloud I was up way up in the sky. And I was feeling some feelings you wouldn\\'t believe. Sometimes I don\\'t believe them myself and I decided I was never coming down.'\n",
        "text2 = 'Just then a tiny little dot caught my eye. It was just about too small to see. But I watched it way too long. It was pulling me down'\n",
        "text3 = 'I was up above it. I was up above it. I was up above it. I was up above it. Now I\\'m down in it'"
      ],
      "metadata": {
        "id": "vWkJHGq0qpRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save them as text files - this is just for class!\n",
        "# ideally you have them as txt first and don't need to do this extra step!\n",
        "\n",
        "# this tells us where to save the file\n",
        "corpus_dir = '/content/'\n",
        "\n",
        "# the \"+\" is creating a single string in the form of /content/text1.txt\n",
        "# the code is creating a text file and putting those strings in the files\n",
        "with open(corpus_dir + 'text1.txt', 'w') as txt:\n",
        "  txt.write(text1)\n",
        "\n",
        "with open(corpus_dir + 'text2.txt', 'w') as txt:\n",
        "  txt.write(text2)\n",
        "\n",
        "with open(corpus_dir + 'text3.txt', 'w') as txt:\n",
        "  txt.write(text3)"
      ],
      "metadata": {
        "id": "T5oOz6__rVvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have some \"texts\" we can create a corpus using NLTK tools."
      ],
      "metadata": {
        "id": "KLcA4Wwpuzww"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import the module to read in plain text\n",
        "from nltk.corpus import PlaintextCorpusReader"
      ],
      "metadata": {
        "id": "jtFU81LrsCGW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create our corpus\n",
        "down_in_it = PlaintextCorpusReader(root = corpus_dir, fileids = ['text1.txt', 'text2.txt', 'text3.txt'])"
      ],
      "metadata": {
        "id": "7_PSvKO0rCA3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we have our own fileids!!\n",
        "down_in_it.fileids()"
      ],
      "metadata": {
        "id": "AmdPGsRXsD7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the corpus does tokenization etc for us already\n",
        "down_in_it.words('text3.txt')"
      ],
      "metadata": {
        "id": "9HgZs55CsF39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What if I want to get the TTR of each text?"
      ],
      "metadata": {
        "id": "ZPlm5riUswjJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# do we understand this loop?\n",
        "\n",
        "for text in ['text1.txt', 'text2.txt', 'text3.txt']:\n",
        "  tokens = [token.lower() for token in down_in_it.words(text)]\n",
        "  ttr = len(set(tokens)) / len(tokens)\n",
        "  print(text)\n",
        "  print(ttr)"
      ],
      "metadata": {
        "id": "_HyiY7xMsJ4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And we can get concordances.."
      ],
      "metadata": {
        "id": "vdQbnyAps3S0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nin_Text = nltk.Text(down_in_it.words())"
      ],
      "metadata": {
        "id": "Jf9IVA1Qs2p5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nin_Text.concordance('i')"
      ],
      "metadata": {
        "id": "-6XXg9imtBk6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}