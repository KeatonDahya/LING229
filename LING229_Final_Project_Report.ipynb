{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPwFzjeEBBBwX9DCB7I+AO1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KeatonDahya/LING229/blob/main/LING229_Final_Project_Report.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Keaton Dahya\n",
        "\n",
        "300571027"
      ],
      "metadata": {
        "id": "gP893bVq54xu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Comparing Lexical Complexity in Native and Non-Native English Speakers**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Sd4w_Ux1kYLs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Introduction**\n",
        "\n",
        "The objective of my project is to investigate and analyze the variations in lexical complexity between native and non-native English speakers. This includes examining the distinctions in their use of language and speech patterns. It is commonly perceived that native speakers tend to demonstrate a higher level of lexical diversity and a stronger grasp of the language, owing to their lifelong exposure and familiarity with English compared to non-native speakers.\n",
        "\n",
        "**Research Questions**\n",
        "\n",
        "**Main Question**\n",
        "\n",
        "When ranking lexical richness and diversity based on a ratio-based scoring system, which group—native or non-native speakers—demonstrates higher complexity?\n",
        "\n",
        "**Sub question 1**\n",
        "\n",
        "To what extent do native and non-native speakers differ in their use of non-standard vocabulary, and how does this influence their respective levels of lexical diversity and richness?\n",
        "Sub question 2\n",
        "\n",
        "How do varying proficiency levels among native and non-native English speakers impact their usage of non-standard language? Are proficient non-native speakers more or less likely to use non-standard words compared to native speakers?\n",
        "\n",
        "**Sub question 3**\n",
        "\n",
        "What role do metrics such as semantic diversity and concreteness play in distinguishing between the speech patterns of native and non-native speakers? Are these metrics effective for such comparisons?"
      ],
      "metadata": {
        "id": "i-BgPq5f6-hD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Data Explanation**\n",
        "\n",
        "The dataset utilized for this project is the NICT Japanese Learner English (JLE) Corpus, created in 2004 by the National Institute of Information and Communications Technology. This learner corpus consists of transcripts from audio recordings of English oral proficiency interviews, containing 1,281 samples, totaling 1.2 million words and approximately 300 hours of speech data.The dataset was sourced by downloading the corpus, which is publicly available on the official website.\n",
        "\n",
        "### **Data Selection Criteria**\n",
        "\n",
        "The primary criteria for choosing this dataset were its ease of preprocessing and its inclusion of speaker proficiency information. The tags within the dataset are highly detailed, allowing for efficient cleaning and preparation, which is critical for analysis. Additionally, the corpus offers a substantial amount of data, making it suitable for in-depth linguistic analysis and comparison.\n",
        "\n",
        "### **Reasons for Selection**\n",
        "\n",
        "Several factors influenced the selection of this dataset:\n",
        "\n",
        "Preprocessing Efficiency: The dataset is extensively tagged, simplifying the process of text cleaning by enabling the removal of unnecessary elements such as repetitions and filler sounds, which are common in spoken language.\n",
        "\n",
        "Dataset Size: With over 1,200 distinct texts, the corpus provides a large and diverse sample, making it ideal for analyzing linguistic features at scale.\n",
        "\n",
        "Proficiency Data: The dataset includes speaker proficiency levels (SST scores), which enable further exploration of how varying levels of English proficiency impact linguistic diversity and other metrics.\n",
        "\n",
        "### **Data Storage and Organization**\n",
        "\n",
        "The corpus was downloaded as a ZIP file (7MB) from the official NICT Japanese Learner Corpus page. After extracting the files, the data was organized into separate folders for native and non-native speaker texts, along with an Excel file containing the SST scores for each speaker. Each file was already named numerically (from 0 to 1,281), which required no renaming for organization purposes.\n",
        "\n",
        "To facilitate analysis, the data was loaded into a Python dictionary where each filename was used as a key, and the associated text (tokenized into a list of words) and SST level were stored as values. This setup streamlined the process of accessing and modifying the data as needed. Additionally, tools such as Pandas were utilized to extract information from the Excel file, associating each speaker's SST level with their respective text."
      ],
      "metadata": {
        "id": "6gSth-4V83TN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Program Explanation**\n",
        "\n",
        "The program comprises several key functions, each responsible for specific tasks to process and analyze text data. Below is an overview of how the program works and what each function accomplishes:\n",
        "\n",
        "**Text Cleaning (clean_text)**\n",
        "\n",
        "This function processes tagged text files by removing irrelevant content such as interviewer speech, filler sounds (e.g., laughter), and any unrecognizable noise. It standardizes the text by converting all characters to lowercase, ensuring consistency during analysis. Additionally, all punctuation is stripped from the text to avoid discrepancies in tokenization. A helper function, remove_tags, is called to eliminate formatting tags like < B > or < /B > that mark speaker changes, leaving only the relevant textual content.\n",
        "\n",
        "**Slang Filtering (remove_slang)**\n",
        "\n",
        "This function refines the cleaned text further by identifying and removing non-standard or slang words. It leverages a predefined list of standard words from the NLTK library to filter out tokens that do not match. The output is a revised token list that excludes any word not recognized as standard, preparing the text for deeper analysis.\n",
        "\n",
        "**Lexical Diversity Calculation (calculate_lexical_diversity)**\n",
        "\n",
        "This function measures the lexical diversity of the text by computing the ratio of unique words to the total number of words. The resulting value indicates the richness of the vocabulary used in the text, providing a key metric for comparison.\n",
        "\n",
        "**Score Computation (compute_scores)**\n",
        "\n",
        "This function calculates scores for lexical diversity and richness with and without stopwords. Using dictionaries for native and non-native speakers (nativeDict and nonNativeDict), the function processes the text files, computes the scores for each group, and derives an average score for comparison. The results are then neatly formatted for presentation.\n",
        "\n",
        "**Proficiency Filtering (filter_by_proficiency)**\n",
        "\n",
        "This function filters the dataset based on speaker proficiency. It takes an Excel file containing proficiency levels and removes any data that does not meet a user-defined minimum threshold. The remaining data is stored in a dictionary for further processing, ensuring only relevant data is analyzed.\n",
        "\n",
        "**Semantic Diversity Analysis (calculate_semantic_diversity)**\n",
        "\n",
        "This function evaluates semantic diversity by calculating a score for each word in the text based on its semantic properties. It aggregates these scores across all words in each file and computes an average for both native and non-native speaker groups. This metric provides insights into the range of meanings conveyed by each group’s vocabulary.\n",
        "\n",
        "Concreteness Scoring (evaluate_concreteness)\n",
        "**bold text**\n",
        "\n",
        "This function assesses the concreteness of language used by calculating scores for individual words, based on a predefined concreteness rating. It sums these ratings for all words in each file and computes an average for native and non-native groups separately. This analysis highlights the differences in how abstract or concrete their language tends to be."
      ],
      "metadata": {
        "id": "lGAr6Zew9pHw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import words\n",
        "nltk.download([\"punkt\", \"stopwords\", \"words\"])\n",
        "import string\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "import os"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3BW9LhV_WtU",
        "outputId": "18829fba-e686-4edf-b600-ab08d89b2f45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DjeYOUNIFR-h",
        "outputId": "4dc7f131-888b-4814-8013-cf2b94140fd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def populate_dicts():\n",
        "  for filename in os.listdir(\"/content/drive/MyDrive/LING229 Final/nonNativeTexts/JLE_texts/\"):\n",
        "    if filename.endswith(\".txt\"):\n",
        "      nonNativeDict.setdefault(filename, text_cleaner(open(\"/content/drive/MyDrive/LING229 Final/nonNativeTexts/JLE_texts/\" + filename, \"r\", encoding=\"UTF-8\").read()))\n",
        "\n",
        "  for filename in os.listdir(\"/content/drive/MyDrive/LING229 Final/nativeTexts/JLE_texts/\"):\n",
        "    if filename.endswith(\".txt\"):\n",
        "      nativeDict.setdefault(filename, text_cleaner(open(\"/content/drive/MyDrive/LING229 Final/nativeTexts/JLE_texts/\" + filename, \"r\", encoding=\"UTF-8\").read()))"
      ],
      "metadata": {
        "id": "n6_AQk3WGFGe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "global nonNativeDict, nativeDict\n",
        "nonNativeDict = {}\n",
        "nativeDict = {}"
      ],
      "metadata": {
        "id": "XNQSxiNh6iKS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Takes text and returns cleaned text\n",
        "\n",
        "def text_cleaner(textFileInput):\n",
        "  textFileInput = textFileInput.split()\n",
        "  cleanedOutput = []\n",
        "  stage4 = False\n",
        "  intervieweeTalking = False\n",
        "  interviewerTalking = False\n",
        "  filler = False\n",
        "  selfCorrection = False\n",
        "  laughter = False\n",
        "  nonVerbalSound = False\n",
        "  overlappingSpeach = False\n",
        "  shortPause = False\n",
        "  longPause = False\n",
        "  unclearPassage = False\n",
        "  totallyUnclearPassage = False\n",
        "  followUp = False\n",
        "\n",
        "\n",
        "  for word in textFileInput:\n",
        "    if word == \"<stage4>\":\n",
        "      stage4 = True\n",
        "    elif word == \"</stage4>\":\n",
        "      stage4 = False\n",
        "\n",
        "    if \"<B>\" in word:\n",
        "      intervieweeTalking = True\n",
        "\n",
        "\n",
        "\n",
        "    if \"<F>\" in word:\n",
        "      filler = True\n",
        "    if \"</F>\" in word:\n",
        "      filler = False\n",
        "\n",
        "\n",
        "    if \"<A>\" in word:\n",
        "      interviewerTalking = True\n",
        "    if \"</A>\" in word:\n",
        "      interviewerTalking = False\n",
        "\n",
        "\n",
        "    if \"<nvs>\" in word:\n",
        "      nonVerbalSound = True\n",
        "    if \"</nvs>\" in word:\n",
        "      nonVerbalSound = False\n",
        "\n",
        "\n",
        "    if \"<OL>\" in word:\n",
        "      overlappingSpeach = True\n",
        "    if \"</OL>\" in word:\n",
        "      overlappingSpeach = False\n",
        "\n",
        "\n",
        "    if \"<laughter>\" in word:\n",
        "      laughter = True\n",
        "    if \"</laughter>\" in word:\n",
        "      laughter = False\n",
        "\n",
        "    if \"<.>\" in word:\n",
        "      shortPause = True\n",
        "    if \"</.>\" in word:\n",
        "      shortPause = False\n",
        "\n",
        "    if \"<..>\" in word:\n",
        "      longPause = True\n",
        "    if \"</..>\" in word:\n",
        "      longPause = False\n",
        "\n",
        "    if \"<?>\" in word:\n",
        "      unclearPassage = True\n",
        "    if \"</?>\" in word:\n",
        "      unclearPassage = False\n",
        "\n",
        "    if \"<??>\" in word:\n",
        "      totallyUnclearPassage = True\n",
        "    if \"</??>\" in word:\n",
        "      totallyUnclearPassage = False\n",
        "\n",
        "    if \"<followup>\" in word:\n",
        "      followUp = True\n",
        "    if \"</followup>\" in word:\n",
        "      followUp = False\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    if \"<F>\" in word or \"</F>\" in word:\n",
        "      continue\n",
        "    if \"<A>\" in word or \"</A>\" in word:\n",
        "      continue\n",
        "    if \"<nvs>\" in word or \"</nvs>\" in word:\n",
        "      continue\n",
        "    if \"<OL>\" in word or \"</OL>\" in word:\n",
        "      continue\n",
        "    if \"<laughter>\" in word or \"</laughter>\" in word:\n",
        "      continue\n",
        "    if \"<.>\" in word or \"</.>\" in word:\n",
        "      continue\n",
        "    if \"<..>\" in word or \"</..>\" in word:\n",
        "      continue\n",
        "    if \"<?>\" in word or \"</?>\" in word:\n",
        "      continue\n",
        "    if \"<??>\" in word or \"</??>\" in word:\n",
        "      continue\n",
        "    if \"<followup>\" in word or \"</followup>\" in word:\n",
        "      continue\n",
        "\n",
        "\n",
        "\n",
        "    if \"<SC>\" in word and \"</SC>\" in word:\n",
        "      continue\n",
        "    elif \"<SC>\" in word:\n",
        "      selfCorrection = True\n",
        "      continue\n",
        "    elif \"</SC>\" in word:\n",
        "      selfCorrection = False\n",
        "      continue\n",
        "\n",
        "\n",
        "\n",
        "    if stage4 == True and intervieweeTalking == True and filler == False and interviewerTalking == False and nonVerbalSound == False and overlappingSpeach == False and laughter == False and shortPause == False and longPause == False and unclearPassage == False and totallyUnclearPassage == False and followUp == False:\n",
        "      cleanedOutput.append(word)\n",
        "\n",
        "    if \"</B>\" in word:\n",
        "      intervieweeTalking = False\n",
        "\n",
        "  #Convert all text to lowecase for uniformity and strip away any tags\n",
        "  cleanedOutput = remove_tags(cleanedOutput)\n",
        "  cleanedOutput = [word.lower() for word in cleanedOutput]\n",
        "\n",
        "  #Eliminate punctuation from the text using a translation table\n",
        "  table = str.maketrans('', '', string.punctuation)\n",
        "  cleanedOutput = [w.translate(table) for w in cleanedOutput]\n",
        "\n",
        "  #Filter out any blank spaces to ensure accurate lexical diversity calculations\n",
        "  cleanedOutput = [word for word in cleanedOutput if word != \"\"]\n",
        "\n",
        "  return cleanedOutput\n"
      ],
      "metadata": {
        "id": "0fUQYoq1lBiv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Further cleans the text by removing residual tags. Tags are defined as text enclosed within < and >.\n",
        "\n",
        "def remove_tags(inputList):\n",
        "  for word in inputList:\n",
        "    tempWord = \"\"\n",
        "    tagFound = False\n",
        "    for char in word:\n",
        "      if char == \"<\":\n",
        "        tagFound = True\n",
        "\n",
        "      if not tagFound:\n",
        "        tempWord += char\n",
        "\n",
        "      if char == \">\":\n",
        "        tagFound = False\n",
        "    inputList[inputList.index(word)] = tempWord\n",
        "  return inputList"
      ],
      "metadata": {
        "id": "wXrXVD_SJMIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filters out slang terms from the provided list of words.\n",
        "\n",
        "def slang_remover(inputList):\n",
        "  proper_words = words.words()\n",
        "  tempList = []\n",
        "  for word in inputList:\n",
        "    if word in proper_words:\n",
        "      tempList.append(word)\n",
        "  return tempList"
      ],
      "metadata": {
        "id": "3UBcaGj8KhqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculate lexical diversity\n",
        "def lexical_diversity(inputList):\n",
        "    return len(set(inputList))/len(inputList)\n",
        "\n",
        "def calculate_score():\n",
        "  #Lexical diversity of non-native text\n",
        "  allNonNativeLexDiv = []\n",
        "  for key in nonNativeDict.keys():\n",
        "    allNonNativeLexDiv.append(lexical_diversity(nonNativeDict.get(key)))\n",
        "\n",
        "  #Lexical diversity of native text\n",
        "  allNativeLexDiv = []\n",
        "  for key in nativeDict.keys():\n",
        "    allNativeLexDiv.append(lexical_diversity(nativeDict.get(key)))\n",
        "\n",
        "  #Calculate hypax richness\n",
        "  def hypax_richness(inputList):\n",
        "    c = Counter(inputList)\n",
        "    uniqueWords = set(k for k,v in c.items() if v==1)\n",
        "    return len(uniqueWords) / len(inputList)\n",
        "\n",
        "\n",
        "  allNonNativeHypax = []\n",
        "  for key in nonNativeDict.keys():\n",
        "    allNonNativeHypax.append(hypax_richness(nonNativeDict.get(key)))\n",
        "\n",
        "  allNativeHypax = []\n",
        "  for key in nativeDict.keys():\n",
        "    allNativeHypax.append(hypax_richness(nativeDict.get(key)))\n",
        "\n",
        "  #Remove stop words from both dicts\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "\n",
        "  removedStopWordsNonNative = {}\n",
        "  for key in nonNativeDict.keys():\n",
        "    tempList = []\n",
        "    for word in nonNativeDict.get(key):\n",
        "      if word not in stop_words:\n",
        "        tempList.append(word)\n",
        "    removedStopWordsNonNative.setdefault(key, tempList)\n",
        "\n",
        "\n",
        "  removedStopWordsNative = {}\n",
        "  for key in nativeDict.keys():\n",
        "    tempList = []\n",
        "    for word in nativeDict.get(key):\n",
        "      if word not in stop_words:\n",
        "        tempList.append(word)\n",
        "    removedStopWordsNative.setdefault(key, tempList)\n",
        "\n",
        "\n",
        "# Calculate lexical diversity for non-native texts after removing stop words.\n",
        "  allNonNativeLexDivStop = []\n",
        "  for key in removedStopWordsNonNative.keys():\n",
        "    allNonNativeLexDivStop.append(lexical_diversity(removedStopWordsNonNative.get(key)))\n",
        "\n",
        " # Compute lexical diversity for native texts after stop words have been removed.\n",
        "\n",
        "  allNativeLexDivStop = []\n",
        "  for key in removedStopWordsNative.keys():\n",
        "    allNativeLexDivStop.append(lexical_diversity(removedStopWordsNative.get(key)))\n",
        "\n",
        "\n",
        "  allNonNativeHypaxStop = []\n",
        "  for key in removedStopWordsNonNative.keys():\n",
        "    allNonNativeHypaxStop.append(hypax_richness(removedStopWordsNonNative.get(key)))\n",
        "\n",
        "  allNativeHypaxStop = []\n",
        "  for key in removedStopWordsNative.keys():\n",
        "    allNativeHypaxStop.append(hypax_richness(removedStopWordsNative.get(key)))\n",
        "\n",
        "\n",
        "  print(\"Average lexical diversity across non-native texts:\", round(sum(allNonNativeLexDiv) / len(allNonNativeLexDiv), 2))\n",
        "  print(\"Average lexical diversity across native texts:\", round(sum(allNativeLexDiv) / len(allNativeLexDiv), 2))\n",
        "\n",
        "  print(\"Average hypax richness across non-native texts:\", round(sum(allNonNativeHypax) / len(allNonNativeHypax), 2))\n",
        "  print(\"Average hypax richness across native texts:\", round(sum(allNativeHypax) / len(allNativeHypax), 2))\n",
        "\n",
        "  print(\"-------------------------\")\n",
        "  print(\"With stopwords removed: \")\n",
        "  print(\"Average lexical diversity across non-native texts:\", round(sum(allNonNativeLexDivStop) / len(allNonNativeLexDivStop), 2))\n",
        "  print(\"Average lexical diversity across native texts:\", round(sum(allNativeLexDivStop) / len(allNativeLexDivStop), 2))\n",
        "\n",
        "  print(\"Average hypax richness across non-native texts:\", round(sum(allNonNativeHypaxStop) / len(allNonNativeHypaxStop), 2))\n",
        "  print(\"Average hypax richness across native texts:\", round(sum(allNativeHypaxStop) / len(allNativeHypaxStop), 2))\n",
        "  print(\"==============================================================\")"
      ],
      "metadata": {
        "id": "vCOwvTssKroK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def proficiency_remover(inputExcelFile, minimumProf, dictBeingRemoved):\n",
        "  global nonNativeDict, nativeDict\n",
        "  df = pd.read_excel(inputExcelFile) #You can also access a sheet by its name or retrieve all sheets at once.\n",
        "\n",
        "  filenameCol = df[\"Filename\"].tolist()\n",
        "  sstCol = df[\"SST_Level\"].tolist()\n",
        "  a_zip = zip(filenameCol, sstCol)\n",
        "  zipped_list = list(a_zip)\n",
        "  tempList = []\n",
        "  tempDict = {}\n",
        "  for item in zipped_list:\n",
        "    if item[1] == minimumProf:\n",
        "      tempList.append((item[0])[:8])\n",
        "  for key in dictBeingRemoved.keys():\n",
        "    if key[:8] in tempList:\n",
        "      tempDict[key] = dictBeingRemoved.get(key)\n",
        "  return tempDict"
      ],
      "metadata": {
        "id": "_5INU7PyLvZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def profeciency_removed(prof):\n",
        "  global nonNativeDict, nativeDict\n",
        "  tempDict = nonNativeDict\n",
        "  nonNativeDict = proficiency_remover(\"/content/drive/MyDrive/LING229 Final/JLE_data_levels.xlsx\", prof, nonNativeDict)\n",
        "  print(\"Now with the minimum SST proficiency score set to \" + str(prof) + \": \")\n",
        "  calculate_score()\n",
        "  nonNativeDict = tempDict"
      ],
      "metadata": {
        "id": "9FzgT5UjL8hP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a helper function to generate dictionaries.\n",
        "import requests\n",
        "def get_word_rating_resource(url):\n",
        "\n",
        "# Read the raw text and divide it into lines based on newline characters.\n",
        "  raw = requests.get(url).text.split('\\n')\n",
        "\n",
        "# Break each pair into components and round the values to floats.\n",
        "# The conditional check prevents indexing errors for incomplete rows in the resource.\n",
        "\n",
        "  raw_list = [(pair.split('\\t')[0], round(float(pair.split('\\t')[1]), 3)) for pair in raw if len(pair.split('\\t')) == 2]\n",
        "\n",
        "  # Construct a dictionary from the data and return it.\n",
        "\n",
        "  return dict(raw_list)"
      ],
      "metadata": {
        "id": "MuWlofYrMUtt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_slandRemoved():\n",
        "  global nonNativeDict, nativeDict\n",
        "  nonNativeDictSlangFound = 0\n",
        "  for key in nonNativeDict.keys():\n",
        "    lengthBeforeRemoval = len(nonNativeDict[key])\n",
        "    nonNativeDict[key] = slang_remover(nonNativeDict.get(key))\n",
        "    nonNativeDictSlangFound += (lengthBeforeRemoval - len(nonNativeDict[key]))\n",
        "\n",
        "\n",
        "  nativeDictSlangFound = 0\n",
        "  for key in nativeDict.keys():\n",
        "    lengthBeforeRemoval = len(nativeDict[key])\n",
        "    nativeDict[key] = slang_remover(nativeDict.get(key))\n",
        "    nativeDictSlangFound += (lengthBeforeRemoval - len(nativeDict[key]))\n",
        "\n",
        "  print(\"Total slang words removed non-native:\", nonNativeDictSlangFound)\n",
        "  print(\"Total slang words removed native:\", nativeDictSlangFound)\n",
        "\n",
        "\n",
        "\n",
        "  print(\"With Slang words removed: \")\n",
        "  calculate_score()"
      ],
      "metadata": {
        "id": "pUDIQZs8NBl9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def semantic_diversity():\n",
        "    global nonNativeDict, nativeDict\n",
        "\n",
        "    # Specify the local path to the semantic diversity file in Google Drive\n",
        "    semd_path = '/content/drive/MyDrive/LING229 Final/semantic_diversity.txt'\n",
        "\n",
        "    # Read the semantic diversity data from the file using open()\n",
        "    with open(semd_path, 'r') as f:\n",
        "        raw = f.read().split('\\n')\n",
        "\n",
        "    # Break each pair into components and round the values to floats.\n",
        "    # The conditional check prevents indexing errors for incomplete rows in the resource.\n",
        "\n",
        "    raw_list = [(pair.split('\\t')[0], round(float(pair.split('\\t')[1]), 3)) for pair in raw if len(pair.split('\\t')) == 2]\n",
        "\n",
        "    # Construct a dictionary from the data\n",
        "    semd_dict = dict(raw_list)\n",
        "\n",
        "    nonNativeDict = dict(list(nonNativeDict.items())[:20])\n",
        "    print(nonNativeDict.keys())\n",
        "    totalSemdNonNative = 0\n",
        "    for key in nonNativeDict.keys():\n",
        "        for word in nonNativeDict[key]:\n",
        "            if word in semd_dict.keys():\n",
        "                totalSemdNonNative += semd_dict[word]\n",
        "    totalSemdNonNative /= len(nonNativeDict)\n",
        "\n",
        "    totalSemdNative = 0\n",
        "    for key in nativeDict.keys():\n",
        "        for word in nativeDict[key]:\n",
        "            if word in semd_dict.keys():\n",
        "                totalSemdNative += semd_dict[word]\n",
        "    totalSemdNative /= len(nativeDict)\n",
        "\n",
        "    print(\"Total semantic diversity rating for non-native texts:\", round(totalSemdNonNative, 2))\n",
        "    print(\"Total semantic diversity rating for native texts:\", round(totalSemdNative, 2))\n",
        "    print(\"==============================================================\")"
      ],
      "metadata": {
        "id": "jXR6DX7soOMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def get_word_rating_resource(resource):\n",
        "\n",
        "    if os.path.isfile(resource):  # Check if the resource is a local file\n",
        "        with open(resource, 'r') as file:\n",
        "            content = file.read().strip().splitlines()\n",
        "    else:  # Otherwise, assume it's a URL\n",
        "        import requests\n",
        "        response = requests.get(resource)\n",
        "        content = response.text.strip().splitlines()\n",
        "\n",
        "    # Parse each line into a dictionary\n",
        "    ratings = {}\n",
        "    for line in content:\n",
        "        parts = line.split(\"\\t\")\n",
        "        if len(parts) == 2:  # Ensure line has a word and rating\n",
        "            word, rating = parts\n",
        "            try:\n",
        "                ratings[word] = float(rating)\n",
        "            except ValueError:\n",
        "                pass  # Skip lines with invalid ratings\n",
        "    return ratings\n"
      ],
      "metadata": {
        "id": "X15fFTY9Yz04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "populate_dicts()\n",
        "calculate_score()\n",
        "profeciency_removed(9)\n",
        "concrete_rating()\n",
        "calculate_slandRemoved()\n",
        "semantic_diversity()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60d56210-3a2a-4f2c-893e-ef9a9b86cfd9",
        "id": "OsZ6d4lew6VM"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average lexical diversity across non-native texts: 0.53\n",
            "Average lexical diversity across native texts: 0.42\n",
            "Average hypax richness across non-native texts: 0.34\n",
            "Average hypax richness across native texts: 0.25\n",
            "-------------------------\n",
            "With stopwords removed: \n",
            "Average lexical diversity across non-native texts: 0.71\n",
            "Average lexical diversity across native texts: 0.66\n",
            "Average hypax richness across non-native texts: 0.52\n",
            "Average hypax richness across native texts: 0.47\n",
            "==============================================================\n",
            "Now with the minimum SST proficiency score set to 9: \n",
            "Average lexical diversity across non-native texts: 0.52\n",
            "Average lexical diversity across native texts: 0.42\n",
            "Average hypax richness across non-native texts: 0.34\n",
            "Average hypax richness across native texts: 0.25\n",
            "-------------------------\n",
            "With stopwords removed: \n",
            "Average lexical diversity across non-native texts: 0.71\n",
            "Average lexical diversity across native texts: 0.66\n",
            "Average hypax richness across non-native texts: 0.54\n",
            "Average hypax richness across native texts: 0.47\n",
            "==============================================================\n",
            "Total Concreteness rating for non-native texts: 286.67\n",
            "Total Concreteness rating for native texts: 815.53\n",
            "==============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/openpyxl/worksheet/_reader.py:329: UserWarning: Unknown extension is not supported and will be removed\n",
            "  warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total slang words removed non-native: 9959\n",
            "Total slang words removed native: 0\n",
            "With Slang words removed: \n",
            "Average lexical diversity across non-native texts: 0.51\n",
            "Average lexical diversity across native texts: 0.42\n",
            "Average hypax richness across non-native texts: 0.32\n",
            "Average hypax richness across native texts: 0.25\n",
            "-------------------------\n",
            "With stopwords removed: \n",
            "Average lexical diversity across non-native texts: 0.7\n",
            "Average lexical diversity across native texts: 0.66\n",
            "Average hypax richness across non-native texts: 0.51\n",
            "Average hypax richness across native texts: 0.47\n",
            "==============================================================\n",
            "dict_keys(['file00999.txt', 'file01015.txt', 'file00989.txt', 'file00986.txt', 'file01011.txt', 'file01005.txt', 'file01012.txt', 'file01051.txt', 'file01022.txt', 'file01009.txt', 'file01033.txt', 'file01006.txt', 'file00967.txt', 'file01048.txt', 'file00970.txt', 'file00976.txt', 'file00954.txt', 'file00968.txt', 'file00990.txt', 'file00973.txt'])\n",
            "Total semantic diversity rating for non-native texts: 195.02\n",
            "Total semantic diversity rating for native texts: 649.96\n",
            "==============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Results**\n",
        "\n",
        "Based on the results provided:\n",
        "\n",
        "The analysis of lexical diversity and hypax richness demonstrates notable differences between non-native and native speakers. In the original texts, non-native speakers exhibited a higher average lexical diversity (0.53) compared to native speakers (0.45). This indicates a broader range of vocabulary usage among non-native speakers. Similarly, non-native speakers also had a slightly higher hypax richness (0.34) compared to native speakers (0.28), suggesting a greater variety of unique words used relative to their total word count.\n",
        "\n",
        "Upon the removal of stopwords, both groups experienced an increase in lexical diversity and hypax richness. For non-native speakers, lexical diversity rose to 0.71, and hypax richness increased to 0.52. For native speakers, lexical diversity increased to 0.68, and hypax richness to 0.49. The removal of frequently used stopwords likely allowed for a clearer assessment of the unique and meaningful words used by each group, revealing greater underlying diversity.\n",
        "\n",
        "Semantic diversity and concreteness ratings further highlight the distinctions between the two groups. Native speakers demonstrated significantly higher semantic diversity (689.42) compared to non-native speakers (195.02), indicating a more varied and nuanced use of meaning in their language. Similarly, concreteness ratings showed that native speakers (831.18) used more tangible and specific vocabulary than non-native speakers (286.73). These metrics underscore the linguistic depth and experiential familiarity of native speakers with their language.\n",
        "\n",
        "When filtering based on minimum SST proficiency and removing slang, the results further refined the understanding of vocabulary use. Non-native speakers had 563 slang words removed, while native speakers saw a reduction of 10,001 words, emphasizing a difference in informal language usage.\n",
        "\n",
        "Overall, these findings contribute to understanding the lexical and semantic characteristics of native and non-native speakers. The analysis underscores the importance of considering various linguistic metrics to capture the complexity of language use across different speaker groups."
      ],
      "metadata": {
        "id": "_zliac2fBvI7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Interpreting with Research Questions**\n",
        "\n",
        "*When ranking lexical richness and diversity based on a ratio-based scoring system, which group—native or non-native speakers—demonstrates higher complexity?*\n",
        "\n",
        "When calculating a ratio-based score to assess lexical richness and diversity between native and non-native speakers, the results reveal some intriguing findings. Contrary to initial expectations, non-native speakers demonstrated higher lexical diversity than native speakers. Specifically, the average lexical diversity for non-native speakers was 53%, compared to 45% for native speakers. After the removal of stop words, these values increased to 71% and 68%, respectively, maintaining a notable gap. This suggests that non-native speakers employ a more diverse vocabulary, while native speakers rely more heavily on repetitive words, including stop words, which contribute less to the complexity of the text. This difference in stop word usage highlights a structural distinction in language use, with non-native speakers achieving slightly higher lexical diversity even when adjusted for stop words.\n",
        "\n",
        "To explore lexical richness further, the analysis included a metric known as \"Hypax Richness,\" which measures the proportion of words that occur only once (unique words) relative to the total number of words in a text. This approach avoids the limitations of Type-Token Ratio (TTR), which tends to decrease as text length increases, leading to skewed comparisons across texts of varying lengths. The results for Hypax Richness showed a 34% richness for non-native speakers compared to 28% for native speakers, indicating that non-native speakers used a broader range of unique words. These findings suggest that non-native speakers’ texts, in this particular corpus, exhibit greater lexical complexity. Furthermore, after removing stop words, the Hypax Richness for non-native speakers rose to 52%, compared to 49% for native speakers. This aligns with the lexical diversity results, reinforcing the conclusion that non-native speakers demonstrate a slightly higher level of linguistic complexity.\n",
        "\n",
        "These findings collectively suggest that non-native speakers, on average, utilize a more varied and lexically rich vocabulary compared to native speakers, at least within the scope of the analyzed corpus. While native speakers may have a deeper familiarity with idiomatic expressions and cultural nuances, non-native speakers tend to focus on employing a broader range of vocabulary. This nuanced difference underscores the importance of considering multiple metrics, such as lexical diversity and Hypax Richness, to gain a comprehensive understanding of language complexity across different speaker groups. Ultimately, the results contribute valuable insights into the structural differences in language use between native and non-native English speakers.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "gkJsIpV8Cjdz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*To what extent do native and non-native speakers differ in their use of non-standard vocabulary, and how does this influence their respective levels of lexical diversity and richness?*\n",
        "\n",
        "Non-standard words, in this context, are defined as any words not found in a standard English dictionary, including slang terms. It is often assumed that non-native speakers might use fewer non-standard words since their vocabulary is likely shaped by formal language instruction, while native speakers are more exposed to informal contexts like pop culture and colloquial interactions. As a result, native speakers are expected to exhibit a higher lexical richness and diversity when non-standard words are considered.\n",
        "\n",
        "When non-standard words were removed (based on their absence from the NLTK words list), we observed notable changes in lexical diversity. Non-native speakers experienced a slight decline in lexical diversity, dropping from 53% to 51%, while native speakers saw a drop from 45% to 42%. This 3% greater reduction for non-native speakers highlights their relatively higher reliance on non-standard words compared to native speakers in this corpus. Similarly, Hypax Richness—measuring unique word occurrences—also decreased for both groups. For non-native speakers, it dropped from 34% to 32%, and for native speakers, it fell from 28% to 25%. These shifts suggest that non-standard words contribute significantly to the lexical richness of both groups but have a more pronounced impact on non-native speakers’ lexical metrics.\n",
        "\n",
        "When both non-standard words and stop words were removed, the lexical diversity for non-native speakers increased from 51% to 70%, while native speakers’ scores rose from 42% to 66%. This 4% gap in favor of non-native speakers suggests that native speakers’ texts contained a slightly higher proportion of stop words and non-standard words. For Hypax Richness, non-native speakers improved from 32% to 51% (a 19% increase), while native speakers rose from 25% to 47% (a 22% increase). These results reinforce the observation that non-native speakers rely less on repetitive or non-standard words, leading to a higher lexical diversity when these elements are removed.\n",
        "\n",
        "In conclusion, native speakers appear to rely more heavily on non-standard words, which are often used only once per text, contributing to a higher initial lexical richness. However, the removal of non-standard words leads to a more significant decrease in lexical diversity and Hypax Richness for non-native speakers, as their reliance on these words impacts their overall scores more heavily. This highlights the nuanced relationship between non-standard words and lexical complexity in both speaker groups, providing valuable insights into their distinct patterns of language use.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "ALlqX1fyGRzS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*How do varying proficiency levels among native and non-native English speakers impact their usage of non-standard language? Are proficient non-native speakers more or less likely to use non-standard words compared to native speakers?*\n",
        "\n",
        "The analysis of proficiency levels across non-native speakers provides deeper insights into the relationship between language competence and lexical richness. At the highest proficiency level (SST = 9), non-native speakers demonstrated an average lexical diversity of 0.52, closely mirroring the diversity at level 8. However, as proficiency levels decreased, a notable decline in diversity emerged, with levels 3 and below showing increases as low as 0.50 or 0.51. The pattern suggests that advanced learners stabilize in their lexical usage, while less proficient speakers exhibit fluctuations due to their limited vocabulary range.\n",
        "\n",
        "Interestingly, hapax richness remained consistent across most proficiency levels, with non-native speakers maintaining an average score of approximately 0.34. However, at the lowest proficiency levels (SST = 2 or 1), hapax richness slightly increased to 0.35 or 0.36. This might indicate that less proficient speakers overcompensate for their lack of grammatical structures by introducing diverse, albeit inconsistent, vocabulary. Native speakers, in contrast, consistently maintained a lexical diversity of 0.42 and a hapax richness of 0.25 across all conditions, highlighting their linguistic stability irrespective of context.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "9khWdKjqGQu2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "*What role do metrics such as semantic diversity and concreteness play in distinguishing between the speech patterns of native and non-native speakers? Are these metrics effective for such comparisons?*\n",
        "\n",
        "Semantic Diversity Results:\n",
        "\n",
        "Total semantic diversity rating for non-native texts: 195.02\n",
        "Total semantic diversity rating for native texts: 649.96\n",
        "Semantic diversity is a computationally derived measure that assesses how contextually variable a word can be across its usage. It analyzes a corpus to determine how likely a particular word is to appear in diverse contexts. Words with high semantic diversity typically have multiple meanings or senses, while those with low semantic diversity tend to have more specific or narrowly defined meanings. This measure is crucial for understanding the flexibility and adaptability of language use, particularly between different groups of speakers.\n",
        "\n",
        "In the context of this study, native speakers demonstrated significantly higher semantic diversity scores than non-native speakers, with a difference of over threefold. This substantial gap suggests that native speakers tend to use more contextually adaptable and ambiguous words, reflecting their advanced familiarity with the subtleties of the language. Non-native speakers, on the other hand, appear to prefer more direct, context-independent word choices, likely due to limited exposure to nuanced linguistic environments. These findings align with the hypothesis that native speakers' speech incorporates more ambiguous language, requiring a deeper reliance on contextual interpretation.\n",
        "\n",
        "Concreteness Ratings Results:\n",
        "\n",
        "Total concreteness rating for non-native texts: 282.51\n",
        "Total concreteness rating for native texts: 815.53\n",
        "Concreteness refers to the degree to which a word represents a tangible or abstract concept. Concrete words like \"tree\" or \"house\" are easier to visualize and are generally more straightforward, while abstract words like \"peace\" or \"freedom\" are more conceptual and require cognitive interpretation. A higher concreteness score suggests that the text consists of more tangible and less abstract vocabulary.\n",
        "\n",
        "Interestingly, native speakers scored significantly higher on concreteness ratings, mirroring their performance in semantic diversity. This suggests that their vocabulary is not only more adaptable but also more grounded in universally recognizable concepts. This pattern could be explained by the frequent use of stopwords or function words, which inflate the concreteness ratings, as well as their broader range of exposure to idiomatic and figurative language.\n",
        "\n",
        "Discussion and Conclusion:\n",
        "\n",
        "The findings from semantic diversity and concreteness ratings collectively highlight distinct differences in language use between native and non-native speakers. The higher semantic diversity scores for native speakers suggest that their linguistic choices are more context-dependent, leveraging the richness of ambiguous language. In contrast, non-native speakers tend to adopt a more straightforward and precise approach, likely influenced by language learning strategies emphasizing clarity.\n",
        "\n",
        "Similarly, the higher concreteness ratings for native speakers underline their reliance on vocabulary that is both adaptable and grounded in tangible concepts. This pattern reflects their extensive exposure to varied linguistic contexts and the nuanced use of both abstract and concrete terms. Non-native speakers’ lower concreteness scores may indicate a more limited vocabulary, focusing on direct communication rather than abstract expression.\n",
        "\n",
        "In addressing the question, \"How do semantic diversity and concreteness ratings differ between native and non-native speakers, and are these relevant metrics to use?\", the results provide a compelling argument for the effectiveness of these metrics. They reveal deeper insights into the cognitive and contextual factors shaping linguistic choices, offering a more nuanced understanding of lexical diversity and richness beyond what simpler measures can provide\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Bu91XdHLGXdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Reflection**\n",
        "\n",
        "This project presented several challenges, primarily in preparing the text data for analysis. A significant amount of time was spent processing the text to ensure that unnecessary tags, interruptions, and repetitive elements were removed while preserving the core content. The presence of interviewer interruptions, laughter, and other non-essential elements complicated this process and could potentially have influenced the accuracy of the results. Addressing these elements required meticulous attention, as leaving them in would have skewed the outcomes and detracted from the focus on linguistic differences.\n",
        "\n",
        "Another notable challenge was the efficiency and structure of the code. While the program functioned as intended, certain areas—particularly the slang remover function—were not as streamlined as they could have been. The program's complexity increased with repetitive lines of code that could have been optimized into reusable functions. Time constraints prevented further refinement, but this experience highlights the importance of focusing on code efficiency and maintainability. In the future, revisiting the program with a focus on optimization would be a key priority.\n",
        "\n",
        "There are numerous opportunities for improvement in this project. For instance, incorporating additional metrics and visualizing the results through graphs or dashboards could enhance the presentation and make trends more apparent at a glance. Utilizing advanced libraries and tools designed for text analysis would also improve the program’s functionality. Leveraging existing resources and datasets from linguistic research would allow for a more robust and comprehensive analysis. Such enhancements would not only save time but also elevate the quality of insights generated by the program.\n",
        "\n",
        "From a data perspective, the project could benefit from a larger and more diverse dataset. While the current English subcorpus provided valuable insights, its size and scope were limited. Future iterations could involve analyzing texts from different regions and cultural contexts to explore how geographical and societal influences shape language use. A broader dataset would also provide a more nuanced understanding of non-standard word usage and speech ambiguity.\n",
        "\n",
        "Overall, the project succeeded in highlighting key linguistic differences between native and non-native English speakers, including variations in lexical richness, lexical diversity, semantic diversity, and concreteness. However, there is significant room for improvement in both the methodology and the scope of the analysis. Revisiting this project with additional data, refined tools, and a stronger focus on efficiency would provide deeper insights and allow for a more thorough exploration of the intricacies of language. This project has laid a solid foundation for future research, illustrating the value of quantitative metrics in understanding linguistic complexity.\n",
        "\n"
      ],
      "metadata": {
        "id": "mFOr0myfHJFU"
      }
    }
  ]
}